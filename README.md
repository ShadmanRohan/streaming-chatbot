# ChatServer â€” RAG-Powered Chatbot Backend

Django 5 backend with document ingestion, semantic search (with MMR), and chat capabilities.

## Features

- ğŸ“„ **Document Management**: Upload and process text documents
- ğŸ” **Semantic Search**: SBERT embeddings with cosine similarity
- ğŸ¯ **MMR (Maximal Marginal Relevance)**: Diverse, relevant retrieval results
- ğŸ’¬ **Chat Sessions**: Conversation management with message history
- ğŸ§ª **Comprehensive Tests**: Unit tests for all retrieval functions

---

## Quick Start

### 1. Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Or use conda environment
conda activate legal_assistant
pip install -r requirements.txt

# Run migrations
python manage.py migrate

# Create superuser (for admin access)
python manage.py createsuperuser
```

### 2. Run Server

```bash
python manage.py runserver
```

### 3. Health Check

```bash
curl http://localhost:8000/health/
# {"status": "ok"}
```

---

## API Endpoints

### Health

#### GET `/health/`
Health check endpoint.

```bash
curl http://localhost:8000/health/
```

**Response:**
```json
{
  "status": "ok"
}
```

---

### Documents

#### POST `/api/documents/upload/`
Upload and process a document.

```bash
curl -X POST http://localhost:8000/api/documents/upload/ \
  -F "file=@document.txt" \
  -F "auto_process=true"
```

**Response:**
```json
{
  "id": "uuid-here",
  "filename": "document.txt",
  "chunk_count": 5,
  "auto_processed": true,
  "created_at": "2025-10-08T12:00:00Z"
}
```

#### GET `/api/documents/`
List all documents.

```bash
curl http://localhost:8000/api/documents/
```

**Response:**
```json
[
  {
    "id": "uuid",
    "filename": "document.txt",
    "raw_text": "Full document text...",
    "chunk_count": 5,
    "created_at": "2025-10-08T12:00:00Z"
  }
]
```

#### GET `/api/documents/{id}/`
Get a specific document.

```bash
curl http://localhost:8000/api/documents/{document-id}/
```

#### GET `/api/documents/{id}/chunks/`
Get all chunks for a document.

```bash
curl http://localhost:8000/api/documents/{document-id}/chunks/
```

**Response:**
```json
[
  {
    "id": "chunk-uuid",
    "chunk_index": 0,
    "text": "This is chunk text...",
    "has_embedding": true,
    "created_at": "2025-10-08T12:00:00Z"
  }
]
```

---

### Retrieval (Semantic Search)

#### POST `/api/retrieve/`
Search for relevant document chunks using semantic similarity.

**With MMR (recommended for diversity):**
```bash
curl -X POST http://localhost:8000/api/retrieve/ \
  -H "Content-Type: application/json" \
  -d '{
    "query": "machine learning algorithms",
    "top_k": 5,
    "use_mmr": true,
    "lambda_param": 0.5
  }'
```

**Without MMR (pure relevance):**
```bash
curl -X POST http://localhost:8000/api/retrieve/ \
  -H "Content-Type: application/json" \
  -d '{
    "query": "machine learning",
    "top_k": 3,
    "use_mmr": false
  }'
```

**Filter by document:**
```bash
curl -X POST http://localhost:8000/api/retrieve/ \
  -H "Content-Type: application/json" \
  -d '{
    "query": "search text",
    "top_k": 5,
    "document_ids": ["doc-uuid-1", "doc-uuid-2"]
  }'
```

**Parameters:**
- `query` (required): Search query text
- `top_k` (optional, default: 3): Number of results to return
- `use_mmr` (optional, default: true): Enable MMR for diversity
- `lambda_param` (optional, default: 0.5): MMR trade-off (0=diversity, 1=relevance)
- `document_ids` (optional): Filter by specific document IDs

**Response:**
```json
{
  "query": "machine learning",
  "top_k": 3,
  "use_mmr": true,
  "lambda_param": 0.5,
  "results": [
    {
      "score": 0.8532,
      "chunk_id": "chunk-uuid",
      "text": "Machine learning is a subset of AI...",
      "document_id": "doc-uuid",
      "document_filename": "ml-basics.txt",
      "chunk_index": 2
    }
  ]
}
```

---

### Chat Sessions

#### GET `/api/sessions/`
List all chat sessions.

```bash
curl http://localhost:8000/api/sessions/
```

#### POST `/api/sessions/`
Create a new chat session.

```bash
curl -X POST http://localhost:8000/api/sessions/ \
  -H "Content-Type: application/json" \
  -d '{"title": "New Conversation"}'
```

#### GET `/api/sessions/{id}/`
Get a specific session with messages.

```bash
curl http://localhost:8000/api/sessions/{session-id}/
```

---

### Chat (AI Conversation)

#### POST `/api/chat/send/` (Synchronous)
Send a message and get AI response with RAG (Retrieval-Augmented Generation).

#### POST `/api/chat/stream/` (Streaming - SSE)
Send a message and receive AI response via Server-Sent Events (streaming).

**Request:**
```bash
curl -X POST http://localhost:8000/api/chat/send/ \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "session-uuid",
    "message": "What is machine learning?",
    "retrieve": true,
    "top_k": 3,
    "use_mmr": true,
    "model": "gpt-4o-mini"
  }'
```

**Parameters:**
- `session_id` (required): Chat session UUID
- `message` (required): User's message/question
- `retrieve` (optional, default: true): Enable RAG retrieval
- `top_k` (optional, default: 3): Number of chunks to retrieve
- `use_mmr` (optional, default: true): Use MMR for diverse results
- `lambda_param` (optional, default: 0.5): MMR trade-off parameter
- `model` (optional, default: gpt-4o-mini): OpenAI model to use

**Response:**
```json
{
  "session_id": "session-uuid",
  "message_id": "message-uuid",
  "content": "Machine learning is a subset of artificial intelligence...",
  "retrieved_chunks": [
    {
      "text": "ML definition from docs...",
      "score": 0.89,
      "document": "ml_basics.txt",
      "chunk_id": "chunk-uuid"
    }
  ],
  "metadata": {
    "tokens_used": 450,
    "retrieval_count": 3,
    "context_messages": 2,
    "model": "gpt-4o-mini"
  }
}
```

**Error Responses:**
```json
{
  "error": "Session not found",
  "code": "SESSION_NOT_FOUND"
}

{
  "error": "LLM authentication failed. Please check API key configuration.",
  "code": "LLM_AUTH_ERROR"
}

{
  "error": "Rate limit exceeded. Please try again later.",
  "code": "RATE_LIMIT_EXCEEDED"
}
```

**Streaming Example (SSE):**

The `/api/chat/stream/` endpoint returns Server-Sent Events for real-time streaming:

```bash
curl -X POST http://localhost:8000/api/chat/stream/ \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "session-uuid",
    "message": "Explain quantum computing",
    "model": "gpt-4o-mini"
  }'
```

**SSE Response Format:**
```
data: {"type": "delta", "content": "Quantum"}
data: {"type": "delta", "content": " computing"}
data: {"type": "delta", "content": " is..."}
data: {"type": "done", "message_id": "msg-uuid", "chunks": 3}
```

Event types:
- `delta`: Text chunk from LLM (stream these to display)
- `done`: Streaming complete (message saved to DB)
- `error`: Error occurred during streaming

**Browser Demo:**  
Open `static/sse-demo.html` in a browser for a live streaming chat demo.

---

## Management Commands

### Ingest Documents
Process all documents (chunk + embed) in bulk:

```bash
python manage.py ingest_docs
```

This will:
1. Find all documents without embeddings
2. Chunk the text
3. Generate embeddings using SBERT
4. Store in database

---

## Configuration

### Environment Variables

Create a `.env` file (see `.env.example`):

```bash
# Django
SECRET_KEY=your-secret-key
DEBUG=1
ALLOWED_HOSTS=localhost,127.0.0.1

# Database (optional - defaults to SQLite)
DB_NAME=qsl_chatbot
DB_USER=qsl_user
DB_PASSWORD=qsl_pass
DB_HOST=localhost
DB_PORT=5432

# OpenAI (REQUIRED for chat functionality)
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_MODEL=gpt-4o-mini

# Chat Configuration
CHAT_CONTEXT_MESSAGES=10
CHAT_MAX_TOKENS=2000
CHAT_TEMPERATURE=0.7

# RAG Configuration
RAG_ENABLED=true
RAG_TOP_K=3
RAG_USE_MMR=true

# Memory Configuration (Phase 7)
CHAT_MAX_TOKENS_CONTEXT=3000
CHAT_HISTORY_MIN_TURNS=6
SUMMARY_INTERVAL_TURNS=5
```

**Important:** The chat endpoint requires a valid OpenAI API key. Get one from [OpenAI Platform](https://platform.openai.com/api-keys).

**Memory Features:**
- **Token-bounded history**: Limits conversation context by token count (default: 3000 tokens)
- **Minimum turns**: Ensures at least N turns are kept regardless of token limit (default: 6)
- **Auto-summarization**: Creates long-term summaries every N assistant turns (default: 5)

### Database

**SQLite (default):**
- Auto-configured, works out of the box
- Uses `db.sqlite3` file

**PostgreSQL (optional):**
- Set `DB_NAME` env var to enable
- Requires `psycopg2-binary` (already in requirements)
- For pgvector support, install extension in Postgres

---

## Testing

### Run All Tests
```bash
python manage.py test
```

### Run Specific Tests
```bash
# Test retrieval functions
python manage.py test chat.tests.RetrievalTests

# Test MMR algorithm
python manage.py test chat.tests.MMRTests

# Test chunking
python manage.py test chat.tests.ChunkingTests
```

### Test Coverage
- âœ… Document chunking
- âœ… MMR algorithm (with/without diversity)
- âœ… Semantic search (with/without MMR)
- âœ… Document filtering
- âœ… Edge cases (empty corpus, no embeddings)

---

## Architecture

### Components

1. **Document Management** (`chat/models.py`)
   - `Document`: Raw text storage
   - `DocumentChunk`: Text chunks with embeddings

2. **Chunking** (`chat/chunking.py`)
   - Semantic paragraph-based splitting
   - Configurable max length

3. **Embeddings** (`chat/embedding_utils.py`)
   - SBERT model: `all-MiniLM-L6-v2`
   - 384-dimensional vectors
   - Cosine similarity

4. **Retrieval** (`chat/retrieval.py`)
   - Basic similarity search
   - MMR for diversity
   - Document filtering

5. **Chat** (`chat/models.py`)
   - `ChatSession`: Conversation container
   - `ChatMessage`: User/assistant messages

---

## Tech Stack

- **Framework**: Django 5.0.6
- **API**: Django REST Framework 3.15
- **Embeddings**: sentence-transformers (SBERT)
- **Database**: SQLite (default) / PostgreSQL (optional)
- **Python**: 3.10+

---

## Project Structure

```
chatserver/
â”œâ”€â”€ chat/                    # Main app
â”‚   â”œâ”€â”€ models.py           # Database models
â”‚   â”œâ”€â”€ serializers.py      # DRF serializers
â”‚   â”œâ”€â”€ views.py            # API endpoints
â”‚   â”œâ”€â”€ urls.py             # URL routing
â”‚   â”œâ”€â”€ retrieval.py        # Search & MMR
â”‚   â”œâ”€â”€ embedding_utils.py  # Embedding generation
â”‚   â”œâ”€â”€ chunking.py         # Text chunking
â”‚   â”œâ”€â”€ llm.py              # OpenAI integration
â”‚   â”œâ”€â”€ prompts.py          # Prompt engineering
â”‚   â”œâ”€â”€ tests.py            # Unit tests (34 tests)
â”‚   â”œâ”€â”€ langgraph/          # LangGraph orchestration (Phase 5)
â”‚   â”‚   â”œâ”€â”€ graph.py        # Graph definition & execution
â”‚   â”‚   â”œâ”€â”€ state.py        # Shared state schema
â”‚   â”‚   â””â”€â”€ nodes/          # Individual graph nodes
â”‚   â”‚       â”œâ”€â”€ load_history.py
â”‚   â”‚       â”œâ”€â”€ decide_retrieve.py
â”‚   â”‚       â”œâ”€â”€ retrieve.py
â”‚   â”‚       â”œâ”€â”€ synthesize.py
â”‚   â”‚       â”œâ”€â”€ synthesize_stream.py  # Streaming variant
â”‚   â”‚       â””â”€â”€ summarize.py
â”‚   â””â”€â”€ management/
â”‚       â””â”€â”€ commands/
â”‚           â””â”€â”€ ingest_docs.py
â”œâ”€â”€ chatserver/             # Project settings
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ urls.py
â”œâ”€â”€ static/
â”‚   â””â”€â”€ sse-demo.html       # Streaming demo
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ manage.py
â””â”€â”€ README.md
```

---

## Development

### Add New Document
```bash
# Via API
curl -X POST http://localhost:8000/api/documents/upload/ \
  -F "file=@mydoc.txt"

# Via admin
# Go to http://localhost:8000/admin/chat/document/
```

### Test Retrieval
```bash
# Search across all documents
curl -X POST http://localhost:8000/api/retrieve/ \
  -H "Content-Type: application/json" \
  -d '{"query": "your search query", "top_k": 5}'
```

### Admin Interface
```bash
# Access at http://localhost:8000/admin/
# Username/password: Set with createsuperuser
```

---

## Next Steps (Roadmap)

- [x] Phase 0-3: Project setup, models, retrieval, MMR âœ…
- [x] Phase 4: Chat endpoint with LLM integration âœ…
- [x] Phase 5: LangGraph orchestration âœ…
- [x] Phase 6: Streaming responses (SSE) âœ…
- [x] Phase 7: Session memory & summarization âœ…
- [ ] Phase 8: Rate limiting, CORS, observability
- [ ] Phase 9: Documentation & deployment

---

## License

MIT
